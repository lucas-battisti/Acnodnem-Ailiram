{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lightning\n",
    "%pip install optuna\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'custom-pytorch-lightning-module'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 25 (delta 11), reused 14 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (25/25), 20.01 KiB | 853.00 KiB/s, done.\n",
      "Cloning into 'pytorch-blocks'...\n",
      "remote: Enumerating objects: 11, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 11 (delta 3), reused 6 (delta 2), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (11/11), 17.19 KiB | 1.01 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lucas-battisti/custom-pytorch-lightning-module.git\n",
    "!mv \"custom-pytorch-lightning-module/modules.py\" \"modules.py\"\n",
    "!rm -rf \"custom-pytorch-lightning-module\"\n",
    "\n",
    "!git clone https://github.com/lucas-battisti/pytorch-blocks.git\n",
    "!mv \"pytorch-blocks/blocks.py\" \"blocks.py\"\n",
    "!rm -rf \"pytorch-blocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "from modules import RegressionModule as Reg\n",
    "from blocks import LinearBlock, Conv1dBlock\n",
    "from data import Custom_DataModule, CNN1D_Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MaxPool1d, AvgPool1d\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"data/Dataframe_f.csv\", index_col=0).astype('float32')\n",
    "e = pd.read_csv(\"data/Dataframe_e_f.csv\", index_col=0).astype('float32')\n",
    "z = pd.read_csv(\"data/Dataframe_z.csv\", index_col=0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def default_training():\n",
    "    L_IN = 16\n",
    "    C_IN = 1\n",
    "    \n",
    "    dm = Custom_DataModule(xez = (x, e, z),\n",
    "                           dataset_class=CNN1D_Dataset,\n",
    "                           version='no', norm=False,\n",
    "                           batch_size=200, num_workers=1)\n",
    "    \n",
    "    convblock1 = Conv1dBlock(in_channels=C_IN, out_channels=32,\n",
    "                             kernel_size=3,\n",
    "                             activation_function=nn.ReLU,\n",
    "                             pooling_layer=MaxPool1d,\n",
    "                             pooling_layer_args={\"kernel_size\": 2})\n",
    "    \n",
    "    L_convblock1 = convblock1.output_shape(L_IN)['L_out']\n",
    "    \n",
    "    convblock2 = Conv1dBlock(in_channels=32, out_channels=64,\n",
    "                             kernel_size=2,\n",
    "                             activation_function=nn.ReLU,\n",
    "                             pooling_layer=MaxPool1d,\n",
    "                             pooling_layer_args={\"kernel_size\": 2})\n",
    "    \n",
    "    L_convblock2 = convblock2.output_shape(L_convblock1)['L_out']\n",
    "    \n",
    "    dense_input = L_convblock2*64\n",
    "    dense_intermed = int(dense_input*0.75)\n",
    "    \n",
    "    l1 = LinearBlock(in_features=dense_input, out_features=dense_intermed,\n",
    "                     activation_function=nn.ReLU)\n",
    "    \n",
    "    l2 = LinearBlock(in_features=dense_intermed, out_features=dense_intermed,\n",
    "                     activation_function=nn.ReLU)\n",
    "    \n",
    "    l3 = LinearBlock(in_features=dense_intermed, out_features=1)\n",
    "    \n",
    "    sequential = nn.Sequential(convblock1,\n",
    "                               convblock2,\n",
    "                               nn.Flatten(start_dim=1, end_dim=2),\n",
    "                               l1,\n",
    "                               l2,\n",
    "                               l3)\n",
    "    \n",
    "    m = Reg(pytorch_module=sequential,\n",
    "            loss_func=nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam, optimizer_args={\"lr\": 1e-3})\n",
    "    \n",
    "    return m, dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 500\n",
    "\n",
    "m, dm = default_training()\n",
    "\n",
    "trainer = L.Trainer(max_epochs=NUM_EPOCH,\n",
    "                        enable_progress_bar = False, log_every_n_steps=1,\n",
    "                        accelerator=\"auto\", devices=1,\n",
    "                        logger = True)\n",
    "trainer.fit(m, datamodule=dm)\n",
    "trainer.test(m, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    NUM_EPOCH = 3\n",
    "    L_IN = 16\n",
    "    C_IN = 1\n",
    "    \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512, 1024, 2048])\n",
    "    \n",
    "    dm = Custom_DataModule(xez = (x, e, z),\n",
    "                           dataset_class=CNN1D_Dataset,\n",
    "                           version='no', norm=True, #dataset_class kwargs\n",
    "                           batch_size=batch_size, num_workers=1)\n",
    "    \n",
    "    n_conv_layers = trial.suggest_int(\"n_conv_layers\", 1, 3)\n",
    "    \n",
    "    sequential = nn.Sequential()\n",
    "    \n",
    "    in_channels = [C_IN]\n",
    "    out_channels = []\n",
    "    kernel_size = []\n",
    "    padding = []\n",
    "    \n",
    "    batch_norm_conv = trial.suggest_categorical(\"batch_norm_conv\", [None, nn.BatchNorm1d])\n",
    "    \n",
    "    pool_type = trial.suggest_categorical(\"pool_type\", [MaxPool1d, AvgPool1d])\n",
    "    pool_kernel_size = []\n",
    "    pool_padding = []\n",
    "        \n",
    "    length = [L_IN]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_conv_layers):\n",
    "        \n",
    "        out_channels.append(trial.suggest_categorical(\"n_channels{}\".format(i+1), [16, 32, 64, 128]))\n",
    "        kernel_size.append(trial.suggest_int(\"kernel_size{}\".format(i+1), 1, length[i]))\n",
    "        padding.append(trial.suggest_int(\"padding{}\".format(i+1), 0, int(floor(kernel_size[i]/2))))\n",
    "\n",
    "        middle_length = length[i]+2*padding[i]-(kernel_size[i]-1)\n",
    "        \n",
    "        pool_kernel_size.append(trial.suggest_int(\"pool_kernel_size{}\".format(i+1), 1, middle_length))\n",
    "        pool_padding.append(trial.suggest_int(\"pool_padding{}\".format(i+1), 0, int(floor(pool_kernel_size[i]/2))))\n",
    "        \n",
    "        sequential = sequential.append(\n",
    "            Conv1dBlock(in_channels=in_channels[i], out_channels=out_channels[i],\n",
    "                        kernel_size=kernel_size[i], padding=padding[i],\n",
    "                        norm_layer=batch_norm_conv, norm_layer_args={\"num_features\": out_channels[i]},\n",
    "                        activation_function=nn.ReLU,\n",
    "                        pooling_layer=pool_type, pooling_layer_args=\n",
    "                        {\"kernel_size\":pool_kernel_size[i], \"padding\":pool_padding[i]})\n",
    "        )\n",
    "        \n",
    "\n",
    "        length.append(int(sequential[i].output_shape(length[i])['L_out']))\n",
    "        in_channels.append(out_channels[i])\n",
    "    \n",
    "    sequential = sequential.append(nn.Flatten(1, 2))\n",
    "    \n",
    "    first_input = out_channels[-1]*length[-1]\n",
    "        \n",
    "    n_dense_layers = trial.suggest_int(\"n_dense_layers\", 1, 3)\n",
    "    \n",
    "    dense_layers_sizes = [first_input]\n",
    "        \n",
    "    batch_norm_dense = trial.suggest_categorical(\"batch_norm_dense\", [None, nn.BatchNorm1d])\n",
    "    \n",
    "    for i in range(n_dense_layers):\n",
    "        \n",
    "        dense_layers_sizes.append(\n",
    "            trial.suggest_int(\"dense_layer_size{}\".format(i+1), 1, dense_layers_sizes[i]))\n",
    "        \n",
    "        sequential = sequential.append(\n",
    "            LinearBlock(in_features=dense_layers_sizes[i], out_features=dense_layers_sizes[i+1],\n",
    "                        norm_layer=batch_norm_dense, norm_layer_args={\"num_features\": dense_layers_sizes[i+1]},\n",
    "                        activation_function=nn.ReLU))\n",
    "        \n",
    "    sequential = sequential.append(nn.Linear(dense_layers_sizes[-1], 1))\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0, 1e-5)\n",
    "    \n",
    "    plateau_factor = trial.suggest_float(\"plateau_factor\", 0, 1)\n",
    "    \n",
    "    m = Reg(pytorch_module=sequential,\n",
    "            loss_func=nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam, optimizer_args={\"lr\": lr, \"weight_decay\": weight_decay},\n",
    "            lr_scheduler=ReduceLROnPlateau,\n",
    "            lr_scheduler_args={\"mode\": 'min', \"factor\": plateau_factor, \"patience\": 5, \"threshold\": 1e-3},\n",
    "            tb_dir=\"CNN1D_no\")\n",
    "    \n",
    "    trainer = L.Trainer(max_epochs=NUM_EPOCH,\n",
    "                        enable_progress_bar = False, log_every_n_steps=1,\n",
    "                        accelerator=\"auto\", devices=1, callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")], logger = True)\n",
    "    trainer.fit(m, datamodule=dm)\n",
    "\n",
    "    return_float = trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    trainer.test(m, datamodule=dm)\n",
    "\n",
    "    return return_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Conv1dBlock(in_channels=32, out_channels=64, kernel_size=3,\n",
    "            pooling_layer=AvgPool1d, pooling_layer_args=\n",
    "                        {\"kernel_size\":2, \"padding\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a.pool_padding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:53:08,703] A new study created in memory with name: no-name-f3b38064-b6ee-4d57-920d-b4b81d3a3970\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.batchnorm.BatchNorm1d'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.pooling.MaxPool1d'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.pooling.AvgPool1d'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_5195/2374135560.py:78: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
      "/tmp/ipykernel_5195/2374135560.py:79: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  weight_decay = trial.suggest_uniform(\"weight_decay\", 0, 1e-5)\n",
      "/tmp/ipykernel_5195/2374135560.py:81: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  plateau_factor = trial.suggest_uniform(\"plateau_factor\", 0, 1)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 26    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "26        Trainable params\n",
      "0         Non-trainable params\n",
      "26        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "[I 2024-03-18 14:53:21,041] Trial 0 finished with value: 0.37694451212882996 and parameters: {'batch_size': 256, 'n_conv_layers': 3, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 8, 'padding1': 2, 'pool_kernel_size1': 8, 'pool_padding1': 3, 'n_channels2': 32, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 0, 'n_channels3': 64, 'kernel_size3': 1, 'padding3': 0, 'pool_kernel_size3': 1, 'pool_padding3': 0, 'n_dense_layers': 1, 'batch_norm_dense': None, 'dense_layer_size1': 25, 'lr': 0.04957832627644119, 'weight_decay': 3.1902992715192983e-06, 'plateau_factor': 0.41600271181987736}. Best is trial 0 with value: 0.37694451212882996.\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.batchnorm.BatchNorm1d'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.pooling.MaxPool1d'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.pooling.AvgPool1d'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_5195/2374135560.py:78: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
      "/tmp/ipykernel_5195/2374135560.py:79: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  weight_decay = trial.suggest_uniform(\"weight_decay\", 0, 1e-5)\n",
      "/tmp/ipykernel_5195/2374135560.py:81: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  plateau_factor = trial.suggest_uniform(\"plateau_factor\", 0, 1)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 18    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "18        Trainable params\n",
      "0         Non-trainable params\n",
      "18        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:53:24,207] Trial 1 finished with value: 1.3433717489242554 and parameters: {'batch_size': 512, 'n_conv_layers': 1, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.AvgPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 4, 'pool_kernel_size1': 5, 'pool_padding1': 1, 'n_dense_layers': 1, 'batch_norm_dense': None, 'dense_layer_size1': 17, 'lr': 0.0028670011764435842, 'weight_decay': 9.87967116950616e-06, 'plateau_factor': 0.6782826009829322}. Best is trial 0 with value: 0.37694451212882996.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 15    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "15        Trainable params\n",
      "0         Non-trainable params\n",
      "15        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:53:28,676] Trial 2 finished with value: 0.5391815900802612 and parameters: {'batch_size': 128, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 8, 'padding1': 0, 'pool_kernel_size1': 5, 'pool_padding1': 0, 'n_dense_layers': 1, 'batch_norm_dense': None, 'dense_layer_size1': 14, 'lr': 0.00819022653575019, 'weight_decay': 7.915278520520864e-07, 'plateau_factor': 0.9824952806183396}. Best is trial 0 with value: 0.37694451212882996.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 24    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "24        Trainable params\n",
      "0         Non-trainable params\n",
      "24        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:53:35,243] Trial 3 finished with value: 0.8775416016578674 and parameters: {'batch_size': 2048, 'n_conv_layers': 3, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.AvgPool1d'>, 'n_channels1': 128, 'kernel_size1': 3, 'padding1': 1, 'pool_kernel_size1': 13, 'pool_padding1': 1, 'n_channels2': 128, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 1, 'pool_padding2': 0, 'n_channels3': 128, 'kernel_size3': 1, 'padding3': 0, 'pool_kernel_size3': 1, 'pool_padding3': 0, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 23, 'lr': 0.1718991110575113, 'weight_decay': 8.172529395919057e-06, 'plateau_factor': 0.4086246863263472}. Best is trial 0 with value: 0.37694451212882996.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 140   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "140       Trainable params\n",
      "0         Non-trainable params\n",
      "140       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:53:43,387] Trial 4 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 32    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "32        Trainable params\n",
      "0         Non-trainable params\n",
      "32        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:53:46,312] Trial 5 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 44    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "44        Trainable params\n",
      "0         Non-trainable params\n",
      "44        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:53:49,317] Trial 6 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 2     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:54:01,027] Trial 7 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 18    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "18        Trainable params\n",
      "0         Non-trainable params\n",
      "18        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:54:16,311] Trial 8 finished with value: 0.5412482023239136 and parameters: {'batch_size': 32, 'n_conv_layers': 2, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.AvgPool1d'>, 'n_channels1': 64, 'kernel_size1': 3, 'padding1': 1, 'pool_kernel_size1': 13, 'pool_padding1': 1, 'n_channels2': 32, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 1, 'pool_padding2': 0, 'n_dense_layers': 3, 'batch_norm_dense': None, 'dense_layer_size1': 20, 'dense_layer_size2': 17, 'dense_layer_size3': 17, 'lr': 0.13128340545119827, 'weight_decay': 2.444409010612567e-06, 'plateau_factor': 0.004022059626306196}. Best is trial 0 with value: 0.37694451212882996.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 2     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:54:25,882] Trial 9 finished with value: 0.5083630681037903 and parameters: {'batch_size': 64, 'n_conv_layers': 3, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 15, 'padding1': 3, 'pool_kernel_size1': 6, 'pool_padding1': 1, 'n_channels2': 64, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 1, 'pool_padding2': 0, 'n_channels3': 32, 'kernel_size3': 1, 'padding3': 0, 'pool_kernel_size3': 1, 'pool_padding3': 0, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 1, 'lr': 0.6435911834500392, 'weight_decay': 6.581566414392261e-06, 'plateau_factor': 0.3295248689691521}. Best is trial 0 with value: 0.37694451212882996.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 2     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "[I 2024-03-18 14:54:33,392] Trial 10 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 36    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "36        Trainable params\n",
      "0         Non-trainable params\n",
      "36        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:54:44,001] Trial 11 finished with value: 0.4426746666431427 and parameters: {'batch_size': 64, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 15, 'padding1': 6, 'pool_kernel_size1': 7, 'pool_padding1': 3, 'n_channels2': 64, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 1, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 53, 'dense_layer_size2': 35, 'lr': 0.7972542797629895, 'weight_decay': 6.8703020689025775e-06, 'plateau_factor': 0.22746176115223776}. Best is trial 0 with value: 0.37694451212882996.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 35    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "35        Trainable params\n",
      "0         Non-trainable params\n",
      "35        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:54:47,670] Trial 12 finished with value: 0.539781391620636 and parameters: {'batch_size': 1024, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 16, 'padding1': 7, 'pool_kernel_size1': 8, 'pool_padding1': 3, 'n_channels2': 32, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 1, 'n_dense_layers': 2, 'batch_norm_dense': None, 'dense_layer_size1': 47, 'dense_layer_size2': 34, 'lr': 0.8667868008049086, 'weight_decay': 6.54363825853816e-06, 'plateau_factor': 0.20953366462119186}. Best is trial 0 with value: 0.37694451212882996.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 54    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "54        Trainable params\n",
      "0         Non-trainable params\n",
      "54        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:54:52,842] Trial 13 finished with value: 0.36801591515541077 and parameters: {'batch_size': 256, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 11, 'padding1': 5, 'pool_kernel_size1': 16, 'pool_padding1': 8, 'n_channels2': 64, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 1, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 80, 'dense_layer_size2': 53, 'lr': 0.05867565314833008, 'weight_decay': 3.1437389484826325e-06, 'plateau_factor': 0.5891085188664137}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 9     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "9         Trainable params\n",
      "0         Non-trainable params\n",
      "9         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:55:00,753] Trial 14 finished with value: 0.4675566554069519 and parameters: {'batch_size': 256, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 11, 'padding1': 4, 'pool_kernel_size1': 14, 'pool_padding1': 7, 'n_channels2': 16, 'kernel_size2': 2, 'padding2': 1, 'pool_kernel_size2': 3, 'pool_padding2': 1, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 9, 'dense_layer_size2': 8, 'lr': 0.06195791769935755, 'weight_decay': 3.6230884265916773e-06, 'plateau_factor': 0.6188926740306699}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 89    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "89        Trainable params\n",
      "0         Non-trainable params\n",
      "89        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:55:03,684] Trial 15 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 13    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "13        Trainable params\n",
      "0         Non-trainable params\n",
      "13        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:55:07,142] Trial 16 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 16    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:55:12,466] Trial 17 finished with value: 1.3929272890090942 and parameters: {'batch_size': 128, 'n_conv_layers': 1, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 9, 'padding1': 2, 'pool_kernel_size1': 4, 'pool_padding1': 2, 'n_dense_layers': 2, 'batch_norm_dense': None, 'dense_layer_size1': 29, 'dense_layer_size2': 15, 'lr': 0.0010757359478083032, 'weight_decay': 3.5492957231565633e-06, 'plateau_factor': 0.7848016403027861}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 42    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "42        Trainable params\n",
      "0         Non-trainable params\n",
      "42        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "[I 2024-03-18 14:55:19,023] Trial 18 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 54    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "54        Trainable params\n",
      "0         Non-trainable params\n",
      "54        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "[I 2024-03-18 14:55:30,047] Trial 19 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 2     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "[I 2024-03-18 14:55:35,948] Trial 20 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 64    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "64        Trainable params\n",
      "0         Non-trainable params\n",
      "64        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:55:46,502] Trial 21 finished with value: 0.46809759736061096 and parameters: {'batch_size': 64, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 6, 'pool_kernel_size1': 7, 'pool_padding1': 2, 'n_channels2': 64, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 1, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 85, 'dense_layer_size2': 63, 'lr': 0.25643672142654755, 'weight_decay': 6.416874089711898e-06, 'plateau_factor': 0.3242725193666687}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 39    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "39        Trainable params\n",
      "0         Non-trainable params\n",
      "39        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:55:56,867] Trial 22 finished with value: 0.7005587220191956 and parameters: {'batch_size': 64, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 14, 'padding1': 7, 'pool_kernel_size1': 17, 'pool_padding1': 4, 'n_channels2': 64, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 1, 'pool_padding2': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 54, 'dense_layer_size2': 38, 'lr': 0.38124300964595537, 'weight_decay': 3.2757735284873863e-06, 'plateau_factor': 0.23425187846155715}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 25    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "25        Trainable params\n",
      "0         Non-trainable params\n",
      "25        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:56:01,453] Trial 23 finished with value: 0.43876001238822937 and parameters: {'batch_size': 256, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 16, 'padding1': 8, 'pool_kernel_size1': 12, 'pool_padding1': 3, 'n_channels2': 64, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 1, 'pool_padding2': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 38, 'dense_layer_size2': 24, 'lr': 0.10022805186935527, 'weight_decay': 7.721657997215341e-06, 'plateau_factor': 0.14347926789914867}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 10    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "10        Trainable params\n",
      "0         Non-trainable params\n",
      "10        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:56:05,711] Trial 24 finished with value: 0.4435049295425415 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 16, 'padding1': 8, 'pool_kernel_size1': 12, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 9, 'lr': 0.07158509990790426, 'weight_decay': 4.371305643733172e-06, 'plateau_factor': 0.12480691970929622}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 26    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "26        Trainable params\n",
      "0         Non-trainable params\n",
      "26        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:56:08,334] Trial 25 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 51    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "51        Trainable params\n",
      "0         Non-trainable params\n",
      "51        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:56:13,378] Trial 26 finished with value: 0.39629775285720825 and parameters: {'batch_size': 256, 'n_conv_layers': 3, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 9, 'pool_padding1': 3, 'n_channels2': 32, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 0, 'n_channels3': 128, 'kernel_size3': 1, 'padding3': 0, 'pool_kernel_size3': 1, 'pool_padding3': 0, 'n_dense_layers': 3, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 79, 'dense_layer_size2': 67, 'dense_layer_size3': 50, 'lr': 0.019746354473152202, 'weight_decay': 7.807335001981794e-06, 'plateau_factor': 0.0040485088254793855}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 50    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "50        Trainable params\n",
      "0         Non-trainable params\n",
      "50        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:56:18,288] Trial 27 finished with value: 0.4401720464229584 and parameters: {'batch_size': 256, 'n_conv_layers': 3, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 9, 'pool_padding1': 3, 'n_channels2': 32, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 0, 'n_channels3': 128, 'kernel_size3': 1, 'padding3': 0, 'pool_kernel_size3': 1, 'pool_padding3': 0, 'n_dense_layers': 3, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 76, 'dense_layer_size2': 67, 'dense_layer_size3': 49, 'lr': 0.014943695781366274, 'weight_decay': 3.0967131118390735e-06, 'plateau_factor': 0.03304015130671223}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 63    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "63        Trainable params\n",
      "0         Non-trainable params\n",
      "63        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:56:21,685] Trial 28 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 6     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "6         Trainable params\n",
      "0         Non-trainable params\n",
      "6         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "[I 2024-03-18 14:56:26,438] Trial 29 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 108   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "108       Trainable params\n",
      "0         Non-trainable params\n",
      "108       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:56:47,559] Trial 30 finished with value: 0.4817643165588379 and parameters: {'batch_size': 32, 'n_conv_layers': 3, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 32, 'kernel_size1': 10, 'padding1': 3, 'pool_kernel_size1': 6, 'pool_padding1': 2, 'n_channels2': 32, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 0, 'n_channels3': 128, 'kernel_size3': 1, 'padding3': 0, 'pool_kernel_size3': 1, 'pool_padding3': 0, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 107, 'lr': 0.0023193169875062214, 'weight_decay': 4.471441249633978e-06, 'plateau_factor': 0.7198502207404989}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 62    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "62        Trainable params\n",
      "0         Non-trainable params\n",
      "62        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:56:53,095] Trial 31 finished with value: 0.4406290650367737 and parameters: {'batch_size': 256, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 5, 'pool_kernel_size1': 10, 'pool_padding1': 4, 'n_channels2': 64, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 2, 'pool_padding2': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 64, 'dense_layer_size2': 61, 'lr': 0.10264084147840306, 'weight_decay': 7.490809958228413e-06, 'plateau_factor': 0.10771828236957003}. Best is trial 13 with value: 0.36801591515541077.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 7     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "7         Trainable params\n",
      "0         Non-trainable params\n",
      "7         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:56:56,640] Trial 32 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 19    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:57:00,656] Trial 33 finished with value: 0.3616939187049866 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 12, 'pool_padding1': 5, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 18, 'lr': 0.21379722539417023, 'weight_decay': 9.778247799336413e-06, 'plateau_factor': 0.03314446276608453}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 12    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:57:02,513] Trial 34 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 18    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "18        Trainable params\n",
      "0         Non-trainable params\n",
      "18        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:57:06,517] Trial 35 finished with value: 0.4247285723686218 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 10, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 17, 'lr': 0.027362557870302676, 'weight_decay': 9.03274502830786e-06, 'plateau_factor': 0.5987127052682476}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 21    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "21        Trainable params\n",
      "0         Non-trainable params\n",
      "21        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:57:09,755] Trial 36 finished with value: 0.4482504427433014 and parameters: {'batch_size': 512, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.AvgPool1d'>, 'n_channels1': 16, 'kernel_size1': 2, 'padding1': 0, 'pool_kernel_size1': 8, 'pool_padding1': 3, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 20, 'lr': 0.19885329900746737, 'weight_decay': 1.757360881353545e-06, 'plateau_factor': 0.9056619818114849}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 23    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "23        Trainable params\n",
      "0         Non-trainable params\n",
      "23        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:57:13,028] Trial 37 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 27    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "27        Trainable params\n",
      "0         Non-trainable params\n",
      "27        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:57:15,025] Trial 38 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 16    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:57:21,527] Trial 39 finished with value: 0.5916950106620789 and parameters: {'batch_size': 128, 'n_conv_layers': 3, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 6, 'padding1': 1, 'pool_kernel_size1': 12, 'pool_padding1': 5, 'n_channels2': 128, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 1, 'pool_padding2': 0, 'n_channels3': 64, 'kernel_size3': 1, 'padding3': 0, 'pool_kernel_size3': 1, 'pool_padding3': 0, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 15, 'lr': 0.009228174909336766, 'weight_decay': 9.882943608998453e-06, 'plateau_factor': 0.47301811237240177}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 67    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "67        Trainable params\n",
      "0         Non-trainable params\n",
      "67        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:57:24,833] Trial 40 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 18    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "18        Trainable params\n",
      "0         Non-trainable params\n",
      "18        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:57:28,756] Trial 41 finished with value: 0.4540225863456726 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 10, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 17, 'lr': 0.021986796938069635, 'weight_decay': 9.147709158270199e-06, 'plateau_factor': 0.6021695810284299}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 20    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "20        Trainable params\n",
      "0         Non-trainable params\n",
      "20        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    \n",
      "self._shutdown_workers()  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "[I 2024-03-18 14:57:47,282] Trial 42 finished with value: 0.4089997410774231 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 9, 'pool_padding1': 3, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 19, 'lr': 0.04975184721124541, 'weight_decay': 8.652058002753873e-06, 'plateau_factor': 0.7300875455294085}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 12    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:57:51,815] Trial 43 finished with value: 0.45524775981903076 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 9, 'padding1': 2, 'pool_kernel_size1': 9, 'pool_padding1': 2, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 11, 'lr': 0.14478590974973082, 'weight_decay': 8.329076215852925e-06, 'plateau_factor': 0.7103396871307875}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 20    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "20        Trainable params\n",
      "0         Non-trainable params\n",
      "20        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:58:00,444] Trial 44 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 23    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "23        Trainable params\n",
      "0         Non-trainable params\n",
      "23        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:58:04,838] Trial 45 finished with value: 0.365811288356781 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 6, 'padding1': 1, 'pool_kernel_size1': 8, 'pool_padding1': 3, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 22, 'lr': 0.06183659895881695, 'weight_decay': 7.106164857737394e-06, 'plateau_factor': 0.9220462421622557}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 26    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "26        Trainable params\n",
      "0         Non-trainable params\n",
      "26        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:58:08,405] Trial 46 finished with value: 1.0212568044662476 and parameters: {'batch_size': 2048, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 32, 'kernel_size1': 8, 'padding1': 2, 'pool_kernel_size1': 6, 'pool_padding1': 1, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 25, 'lr': 0.371434961013748, 'weight_decay': 7.108862454201085e-06, 'plateau_factor': 0.9512062828134277}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 34    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "34        Trainable params\n",
      "0         Non-trainable params\n",
      "34        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:58:11,526] Trial 47 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 34    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "34        Trainable params\n",
      "0         Non-trainable params\n",
      "34        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:58:14,495] Trial 48 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 2     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "2         Trainable params\n",
      "0         Non-trainable params\n",
      "2         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:58:18,300] Trial 49 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 199   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "199       Trainable params\n",
      "0         Non-trainable params\n",
      "199       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "[I 2024-03-18 14:58:22,722] Trial 50 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 23    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "23        Trainable params\n",
      "0         Non-trainable params\n",
      "23        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:58:27,344] Trial 51 finished with value: 0.39684364199638367 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 9, 'pool_padding1': 3, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 22, 'lr': 0.04908059461720497, 'weight_decay': 8.516028848598221e-06, 'plateau_factor': 0.8451504969911656}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 23    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "23        Trainable params\n",
      "0         Non-trainable params\n",
      "23        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:58:32,928] Trial 52 finished with value: 0.3688001036643982 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 5, 'padding1': 1, 'pool_kernel_size1': 10, 'pool_padding1': 3, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 22, 'lr': 0.04036156472606829, 'weight_decay': 7.854110833793354e-07, 'plateau_factor': 0.9361585817693963}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 25    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "25        Trainable params\n",
      "0         Non-trainable params\n",
      "25        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:58:37,753] Trial 53 finished with value: 0.3791166841983795 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 6, 'padding1': 1, 'pool_kernel_size1': 10, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 24, 'lr': 0.0351945995206197, 'weight_decay': 5.765458413924931e-07, 'plateau_factor': 0.927802049231095}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 25    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "25        Trainable params\n",
      "0         Non-trainable params\n",
      "25        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:58:43,181] Trial 54 finished with value: 0.3728742003440857 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 6, 'padding1': 1, 'pool_kernel_size1': 10, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 24, 'lr': 0.03668752693306149, 'weight_decay': 6.052293353523669e-08, 'plateau_factor': 0.9452006413051215}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 13    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "13        Trainable params\n",
      "0         Non-trainable params\n",
      "13        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:58:47,695] Trial 55 finished with value: 2.1247401237487793 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 7, 'padding1': 1, 'pool_kernel_size1': 11, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 12, 'lr': 3.410639640749186e-05, 'weight_decay': 8.269476234461815e-07, 'plateau_factor': 0.8881154977439257}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 24    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "24        Trainable params\n",
      "0         Non-trainable params\n",
      "24        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:05,170] Trial 56 finished with value: 0.49471691250801086 and parameters: {'batch_size': 64, 'n_conv_layers': 1, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 3, 'padding1': 0, 'pool_kernel_size1': 13, 'pool_padding1': 6, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 23, 'lr': 0.19627377632528528, 'weight_decay': 2.029421529036899e-07, 'plateau_factor': 0.9646640611619492}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 31    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "31        Trainable params\n",
      "0         Non-trainable params\n",
      "31        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:20,717] Trial 57 finished with value: 0.4320615828037262 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 32, 'kernel_size1': 9, 'padding1': 2, 'pool_kernel_size1': 8, 'pool_padding1': 2, 'n_dense_layers': 1, 'batch_norm_dense': None, 'dense_layer_size1': 30, 'lr': 0.06821071702829819, 'weight_decay': 1.0010807003836504e-06, 'plateau_factor': 0.8218834403706538}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 21    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "21        Trainable params\n",
      "0         Non-trainable params\n",
      "21        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:34,831] Trial 58 finished with value: 0.6756148934364319 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 7, 'padding1': 1, 'pool_kernel_size1': 10, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 20, 'lr': 0.49740553265318604, 'weight_decay': 1.3906727633681964e-06, 'plateau_factor': 0.8919096303819233}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 6     \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "6         Trainable params\n",
      "0         Non-trainable params\n",
      "6         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:39,018] Trial 59 finished with value: 0.5741905570030212 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 12, 'padding1': 4, 'pool_kernel_size1': 12, 'pool_padding1': 5, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 5, 'lr': 0.012860436510896864, 'weight_decay': 2.6683469104295326e-06, 'plateau_factor': 0.8148301661332926}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 99    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "99        Trainable params\n",
      "0         Non-trainable params\n",
      "99        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:46,194] Trial 60 finished with value: 0.6215030550956726 and parameters: {'batch_size': 128, 'n_conv_layers': 2, 'batch_norm_conv': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 6, 'padding1': 1, 'pool_kernel_size1': 11, 'pool_padding1': 3, 'n_channels2': 128, 'kernel_size2': 1, 'padding2': 0, 'pool_kernel_size2': 1, 'pool_padding2': 0, 'n_dense_layers': 1, 'batch_norm_dense': None, 'dense_layer_size1': 98, 'lr': 0.0036362807638162568, 'weight_decay': 5.316474060886725e-07, 'plateau_factor': 0.9349594927396048}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 25    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "25        Trainable params\n",
      "0         Non-trainable params\n",
      "25        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:50,503] Trial 61 finished with value: 0.4044928550720215 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 6, 'padding1': 1, 'pool_kernel_size1': 10, 'pool_padding1': 4, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 24, 'lr': 0.03173605031716561, 'weight_decay': 7.019183663083616e-08, 'plateau_factor': 0.9240204501213544}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 27    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "27        Trainable params\n",
      "0         Non-trainable params\n",
      "27        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:54,683] Trial 62 finished with value: 0.5300116539001465 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 4, 'padding1': 1, 'pool_kernel_size1': 8, 'pool_padding1': 3, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 26, 'lr': 0.10542094511242686, 'weight_decay': 1.1617990654020666e-06, 'plateau_factor': 0.9949677821405892}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 128   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "128       Trainable params\n",
      "0         Non-trainable params\n",
      "128       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 14:59:59,088] Trial 63 finished with value: 0.37571027874946594 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 8, 'padding1': 2, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 127, 'lr': 0.0390970879068295, 'weight_decay': 2.332328383389881e-06, 'plateau_factor': 0.8737216226439877}. Best is trial 33 with value: 0.3616939187049866.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 43    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "43        Trainable params\n",
      "0         Non-trainable params\n",
      "43        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:00:03,404] Trial 64 finished with value: 0.35688820481300354 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 8, 'padding1': 3, 'pool_kernel_size1': 3, 'pool_padding1': 0, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 42, 'lr': 0.06562949492693816, 'weight_decay': 2.18180418414685e-06, 'plateau_factor': 0.8748450844150553}. Best is trial 64 with value: 0.35688820481300354.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 28    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "28        Trainable params\n",
      "0         Non-trainable params\n",
      "28        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:00:07,402] Trial 65 finished with value: 0.3783458173274994 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 8, 'padding1': 3, 'pool_kernel_size1': 4, 'pool_padding1': 0, 'n_dense_layers': 1, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 27, 'lr': 0.25590238755797395, 'weight_decay': 2.0758514802993377e-06, 'plateau_factor': 0.7963193750757391}. Best is trial 64 with value: 0.35688820481300354.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 101   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "101       Trainable params\n",
      "0         Non-trainable params\n",
      "101       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:00:12,115] Trial 66 finished with value: 0.3308209478855133 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 10, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 132, 'dense_layer_size2': 100, 'lr': 0.016685418189398733, 'weight_decay': 1.7860943230606704e-06, 'plateau_factor': 0.8544689875743607}. Best is trial 66 with value: 0.3308209478855133.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 157   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "157       Trainable params\n",
      "0         Non-trainable params\n",
      "157       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "[I 2024-03-18 15:00:16,066] Trial 67 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 96    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "96        Trainable params\n",
      "0         Non-trainable params\n",
      "96        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:00:20,990] Trial 68 finished with value: 0.3200305104255676 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 12, 'padding1': 4, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 133, 'dense_layer_size2': 95, 'lr': 0.014662054727685114, 'weight_decay': 6.314860033574479e-07, 'plateau_factor': 0.7657990162880994}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 94    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "94        Trainable params\n",
      "0         Non-trainable params\n",
      "94        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 15:00:25,097] Trial 69 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 31    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "31        Trainable params\n",
      "0         Non-trainable params\n",
      "31        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:00:34,712] Trial 70 finished with value: 0.37364664673805237 and parameters: {'batch_size': 64, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 11, 'padding1': 4, 'pool_kernel_size1': 2, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 47, 'dense_layer_size2': 30, 'lr': 0.01724297894434071, 'weight_decay': 1.9704810171594752e-06, 'plateau_factor': 0.6676976915417885}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 95    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "95        Trainable params\n",
      "0         Non-trainable params\n",
      "95        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff2302f28e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:00:47,445] Trial 71 finished with value: 0.362863689661026 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 10, 'padding1': 3, 'pool_kernel_size1': 2, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 95, 'dense_layer_size2': 94, 'lr': 0.00970220774634412, 'weight_decay': 4.04088712061971e-07, 'plateau_factor': 0.9165081554895106}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 97    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "97        Trainable params\n",
      "0         Non-trainable params\n",
      "97        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:00:55,819] Trial 72 finished with value: 0.33721956610679626 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 4, 'pool_kernel_size1': 2, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 96, 'dense_layer_size2': 96, 'lr': 0.010130247556477803, 'weight_decay': 4.6861783503962955e-07, 'plateau_factor': 0.8292721102196496}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 97    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "97        Trainable params\n",
      "0         Non-trainable params\n",
      "97        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:01,069] Trial 73 finished with value: 0.3558926582336426 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 4, 'pool_kernel_size1': 2, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 96, 'dense_layer_size2': 96, 'lr': 0.010083644343948489, 'weight_decay': 6.389723682503282e-07, 'plateau_factor': 0.835133626016466}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 81    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "81        Trainable params\n",
      "0         Non-trainable params\n",
      "81        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:06,070] Trial 74 finished with value: 0.36447304487228394 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 14, 'padding1': 4, 'pool_kernel_size1': 2, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 80, 'dense_layer_size2': 80, 'lr': 0.009031361734750026, 'weight_decay': 4.7268118658630994e-07, 'plateau_factor': 0.8315035961285475}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 81    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "81        Trainable params\n",
      "0         Non-trainable params\n",
      "81        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 15:01:08,388] Trial 75 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 65    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "65        Trainable params\n",
      "0         Non-trainable params\n",
      "65        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:13,237] Trial 76 finished with value: 0.35743477940559387 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 4, 'pool_kernel_size1': 3, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 64, 'dense_layer_size2': 64, 'lr': 0.009248209216957421, 'weight_decay': 4.0601883600476863e-07, 'plateau_factor': 0.7877145877041912}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 103   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "103       Trainable params\n",
      "0         Non-trainable params\n",
      "103       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 15:01:15,789] Trial 77 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 111   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "111       Trainable params\n",
      "0         Non-trainable params\n",
      "111       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:20,591] Trial 78 finished with value: 0.3502649962902069 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 15, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 126, 'dense_layer_size2': 110, 'lr': 0.009669980028693483, 'weight_decay': 1.3801453551530402e-06, 'plateau_factor': 0.7594234445571572}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 37    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 15:01:23,408] Trial 79 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 116   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "116       Trainable params\n",
      "0         Non-trainable params\n",
      "116       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:28,852] Trial 80 finished with value: 0.4354652762413025 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.AvgPool1d'>, 'n_channels1': 16, 'kernel_size1': 12, 'padding1': 5, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 140, 'dense_layer_size2': 115, 'lr': 0.0019815858131408434, 'weight_decay': 1.1195832190913619e-06, 'plateau_factor': 0.7490782519708359}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 97    \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "97        Trainable params\n",
      "0         Non-trainable params\n",
      "97        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:33,495] Trial 81 finished with value: 0.9927521347999573 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 10, 'padding1': 3, 'pool_kernel_size1': 2, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 96, 'dense_layer_size2': 96, 'lr': 0.0006569251712124751, 'weight_decay': 3.8849651458398287e-07, 'plateau_factor': 0.8596818921359256}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 113   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "113       Trainable params\n",
      "0         Non-trainable params\n",
      "113       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:38,416] Trial 82 finished with value: 0.34365054965019226 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 128, 'dense_layer_size2': 112, 'lr': 0.00981907476877204, 'weight_decay': 8.842276436833736e-07, 'plateau_factor': 0.7705734982601916}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 117   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "117       Trainable params\n",
      "0         Non-trainable params\n",
      "117       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:43,251] Trial 83 finished with value: 0.3326544761657715 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 132, 'dense_layer_size2': 116, 'lr': 0.01892912062313574, 'weight_decay': 1.70093724720304e-06, 'plateau_factor': 0.769656980208838}. Best is trial 68 with value: 0.3200305104255676.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 120   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "120       Trainable params\n",
      "0         Non-trainable params\n",
      "120       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:01:48,211] Trial 84 finished with value: 0.301215261220932 and parameters: {'batch_size': 256, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 13, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 133, 'dense_layer_size2': 119, 'lr': 0.01837967739177978, 'weight_decay': 1.709789504734745e-06, 'plateau_factor': 0.7758650453398611}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 119   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "119       Trainable params\n",
      "0         Non-trainable params\n",
      "119       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:02:04,069] Trial 85 finished with value: 0.3482998013496399 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 16, 'kernel_size1': 15, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 128, 'dense_layer_size2': 118, 'lr': 0.019490768232437113, 'weight_decay': 1.7319245654642932e-06, 'plateau_factor': 0.7631054697268572}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 121   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "121       Trainable params\n",
      "0         Non-trainable params\n",
      "121       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "[I 2024-03-18 15:02:13,284] Trial 86 pruned. Trial was pruned at epoch 1.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 131   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "131       Trainable params\n",
      "0         Non-trainable params\n",
      "131       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:02:33,638] Trial 87 finished with value: 0.34779417514801025 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 14, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 220, 'dense_layer_size2': 130, 'lr': 0.0047763949152275, 'weight_decay': 1.560295282835702e-06, 'plateau_factor': 0.7565264497307851}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 133   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "133       Trainable params\n",
      "0         Non-trainable params\n",
      "133       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:03:16,608] Trial 88 finished with value: 0.3598998188972473 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 14, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 215, 'dense_layer_size2': 132, 'lr': 0.005289148684633856, 'weight_decay': 1.5929339622532971e-06, 'plateau_factor': 0.6916763865608662}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 112   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "112       Trainable params\n",
      "0         Non-trainable params\n",
      "112       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:03:36,149] Trial 89 finished with value: 0.5233197212219238 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 16, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 182, 'dense_layer_size2': 111, 'lr': 0.021254159178130894, 'weight_decay': 2.6530533550560384e-06, 'plateau_factor': 0.7679409004004158}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 137   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "137       Trainable params\n",
      "0         Non-trainable params\n",
      "137       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:03:56,122] Trial 90 finished with value: 0.3323454260826111 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 14, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 242, 'dense_layer_size2': 136, 'lr': 0.003872134721568153, 'weight_decay': 1.3990436630229946e-06, 'plateau_factor': 0.7209165043888561}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 139   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "139       Trainable params\n",
      "0         Non-trainable params\n",
      "139       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:04:16,481] Trial 91 finished with value: 0.36060354113578796 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 14, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 255, 'dense_layer_size2': 138, 'lr': 0.0038910230019711368, 'weight_decay': 1.363681950542972e-06, 'plateau_factor': 0.7307150557993931}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 189   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "189       Trainable params\n",
      "0         Non-trainable params\n",
      "189       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "[I 2024-03-18 15:04:41,392] Trial 92 finished with value: 0.368547648191452 and parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 15, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 598, 'dense_layer_size2': 188, 'lr': 0.007675057193722235, 'weight_decay': 1.8470766530230278e-06, 'plateau_factor': 0.7654551201367935}. Best is trial 84 with value: 0.301215261220932.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | pytorch_module | Sequential | 133   \n",
      "1 | loss_func      | MSELoss    | 0     \n",
      "----------------------------------------------\n",
      "133       Trainable params\n",
      "0         Non-trainable params\n",
      "133       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "[W 2024-03-18 15:04:47,325] Trial 93 failed with parameters: {'batch_size': 32, 'n_conv_layers': 1, 'batch_norm_conv': None, 'pool_type': <class 'torch.nn.modules.pooling.MaxPool1d'>, 'n_channels1': 128, 'kernel_size1': 12, 'padding1': 3, 'pool_kernel_size1': 1, 'pool_padding1': 0, 'n_dense_layers': 2, 'batch_norm_dense': <class 'torch.nn.modules.batchnorm.BatchNorm1d'>, 'dense_layer_size1': 243, 'dense_layer_size2': 132, 'lr': 0.01513314881447926, 'weight_decay': 8.918189853529411e-07, 'plateau_factor': 0.8109707009809872} because of the following error: RuntimeError(\"Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5195/2374135560.py\", line 93, in objective\n",
      "    trainer.fit(m, datamodule=dm)\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 987, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1033, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 141, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 128, in run\n",
      "    batch, batch_idx, dataloader_idx = next(data_fetcher)\n",
      "                                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py\", line 133, in __next__\n",
      "    batch = super().__next__()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py\", line 60, in __next__\n",
      "    batch = next(self.iterator)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 341, in __next__\n",
      "    out = next(self._iterator)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 142, in __next__\n",
      "    out = next(self.iterators[0])\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1329, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1295, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/battisti/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1160, in _try_get_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code\n",
      "[W 2024-03-18 15:04:47,333] Trial 93 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pruner \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mHyperbandPruner()\n\u001b[1;32m      3\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1800\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     83\u001b[0m m \u001b[38;5;241m=\u001b[39m Reg(pytorch_module\u001b[38;5;241m=\u001b[39msequential,\n\u001b[1;32m     84\u001b[0m         loss_func\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss,\n\u001b[1;32m     85\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam, optimizer_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: lr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m: weight_decay},\n\u001b[1;32m     86\u001b[0m         lr_scheduler\u001b[38;5;241m=\u001b[39mReduceLROnPlateau,\n\u001b[1;32m     87\u001b[0m         lr_scheduler_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactor\u001b[39m\u001b[38;5;124m\"\u001b[39m: plateau_factor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-3\u001b[39m},\n\u001b[1;32m     88\u001b[0m         tb_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN1D_no\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCH,\n\u001b[1;32m     91\u001b[0m                     enable_progress_bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     92\u001b[0m                     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[PyTorchLightningPruningCallback(trial, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)], logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 93\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m return_float \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     97\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(m, datamodule\u001b[38;5;241m=\u001b[39mdm)\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_accumulate():\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_first_loop_iter \u001b[38;5;241m=\u001b[39m first_loop_iter\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:128\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/Acnodnem-Ailiram/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1160\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEMFILE:\n\u001b[0;32m-> 1160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many open files. Communication with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m workers is no longer possible. Please increase the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1163\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m limit using `ulimit -n` in the shell or change the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1164\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sharing strategy by calling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1165\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch.multiprocessing.set_sharing_strategy(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at the beginning of your code\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Too many open files. Communication with the workers is no longer possible. Please increase the limit using `ulimit -n` in the shell or change the sharing strategy by calling `torch.multiprocessing.set_sharing_strategy('file_system')` at the beginning of your code"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.HyperbandPruner()\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=150, timeout=1800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Acnodnem-Ailiram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
