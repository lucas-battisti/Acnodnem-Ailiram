{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lightning\n",
    "%pip install optuna\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'custom-pytorch-lightning-module'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 25 (delta 11), reused 14 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (25/25), 20.01 KiB | 853.00 KiB/s, done.\n",
      "Cloning into 'pytorch-blocks'...\n",
      "remote: Enumerating objects: 11, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 11 (delta 3), reused 6 (delta 2), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (11/11), 17.19 KiB | 1.01 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lucas-battisti/custom-pytorch-lightning-module.git\n",
    "!mv \"custom-pytorch-lightning-module/modules.py\" \"modules.py\"\n",
    "!rm -rf \"custom-pytorch-lightning-module\"\n",
    "\n",
    "!git clone https://github.com/lucas-battisti/pytorch-blocks.git\n",
    "!mv \"pytorch-blocks/blocks.py\" \"blocks.py\"\n",
    "!rm -rf \"pytorch-blocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "from modules import RegressionModule as Reg\n",
    "from blocks import LinearBlock, Conv1dBlock\n",
    "from data import Custom_DataModule, CNN1D_Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MaxPool1d, AvgPool1d\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"data/Dataframe_f.csv\", index_col=0).astype('float32')\n",
    "e = pd.read_csv(\"data/Dataframe_e_f.csv\", index_col=0).astype('float32')\n",
    "z = pd.read_csv(\"data/Dataframe_z.csv\", index_col=0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def default_training():\n",
    "    L_IN = 16\n",
    "    C_IN = 1\n",
    "    \n",
    "    dm = Custom_DataModule(xez = (x, e, z),\n",
    "                           dataset_class=CNN1D_Dataset,\n",
    "                           version='no', norm=False,\n",
    "                           batch_size=200, num_workers=1)\n",
    "    \n",
    "    convblock1 = Conv1dBlock(in_channels=C_IN, out_channels=32,\n",
    "                             kernel_size=3,\n",
    "                             activation_function=nn.ReLU,\n",
    "                             pooling_layer=MaxPool1d,\n",
    "                             pooling_layer_args={\"kernel_size\": 2})\n",
    "    \n",
    "    L_convblock1 = convblock1.output_shape(L_IN)['L_out']\n",
    "    \n",
    "    convblock2 = Conv1dBlock(in_channels=32, out_channels=64,\n",
    "                             kernel_size=2,\n",
    "                             activation_function=nn.ReLU,\n",
    "                             pooling_layer=MaxPool1d,\n",
    "                             pooling_layer_args={\"kernel_size\": 2})\n",
    "    \n",
    "    L_convblock2 = convblock2.output_shape(L_convblock1)['L_out']\n",
    "    \n",
    "    dense_input = L_convblock2*64\n",
    "    dense_intermed = int(dense_input*0.75)\n",
    "    \n",
    "    l1 = LinearBlock(in_features=dense_input, out_features=dense_intermed,\n",
    "                     activation_function=nn.ReLU)\n",
    "    \n",
    "    l2 = LinearBlock(in_features=dense_intermed, out_features=dense_intermed,\n",
    "                     activation_function=nn.ReLU)\n",
    "    \n",
    "    l3 = LinearBlock(in_features=dense_intermed, out_features=1)\n",
    "    \n",
    "    sequential = nn.Sequential(convblock1,\n",
    "                               convblock2,\n",
    "                               nn.Flatten(start_dim=1, end_dim=2),\n",
    "                               l1,\n",
    "                               l2,\n",
    "                               l3)\n",
    "    \n",
    "    m = Reg(pytorch_module=sequential,\n",
    "            loss_func=nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam, optimizer_args={\"lr\": 1e-3})\n",
    "    \n",
    "    return m, dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 500\n",
    "\n",
    "m, dm = default_training()\n",
    "\n",
    "trainer = L.Trainer(max_epochs=NUM_EPOCH,\n",
    "                        enable_progress_bar = False, log_every_n_steps=1,\n",
    "                        accelerator=\"auto\", devices=1,\n",
    "                        logger = True)\n",
    "trainer.fit(m, datamodule=dm)\n",
    "trainer.test(m, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    NUM_EPOCH = 300\n",
    "    L_IN = 16\n",
    "    C_IN = 1\n",
    "    \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512, 1024, 2048])\n",
    "    \n",
    "    dm = Custom_DataModule(xez = (x, e, z),\n",
    "                           dataset_class=CNN1D_Dataset,\n",
    "                           version='no', norm=False, #dataset_class kwargs\n",
    "                           batch_size=batch_size, num_workers=1)\n",
    "    \n",
    "    n_conv_layers = trial.suggest_int(\"n_conv_layers\", 1, 3)\n",
    "    \n",
    "    sequential = nn.Sequential()\n",
    "    \n",
    "    in_channels = [C_IN]\n",
    "    out_channels = []\n",
    "    kernel_size = []\n",
    "    padding = []\n",
    "    \n",
    "    batch_norm_conv = trial.suggest_categorical(\"batch_norm_conv\", [None, nn.BatchNorm1d])\n",
    "    \n",
    "    pool_type = trial.suggest_categorical(\"pool_type\", [MaxPool1d, AvgPool1d])\n",
    "    pool_kernel_size = []\n",
    "    pool_padding = []\n",
    "        \n",
    "    length = [L_IN]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_conv_layers):\n",
    "        \n",
    "        out_channels.append(trial.suggest_categorical(\"n_channels{}\".format(i+1), [16, 32, 64, 128]))\n",
    "        kernel_size.append(trial.suggest_int(\"kernel_size{}}\".format(i+1), 1, length[i]))\n",
    "        padding.append(trial.suggest_int(\"padding{}\".format{i+1}, 0, int(floor(kernel_size[i]/2))))\n",
    "        \n",
    "        middle_length = length[i]+2*padding[i]-(kernel_size[i]-1)\n",
    "        \n",
    "        pool_kernel_size.append(trial.suggest_int(\"pool_kernel_size{}}\".format(i+1), 1, middle_length))\n",
    "        pool_padding.append(trial.suggest_int(\"padding{}\".format{i+1}, 0, int(floor(pool_kernel_size[i]/2))))\n",
    "        \n",
    "        sequential = sequential.append(\n",
    "            Conv1dBlock(in_channels=in_channels[i], out_channels=out_channels[i],\n",
    "                        kernel_size=kernel_size[i], padding=padding[i],\n",
    "                        norm_layer=batch_norm_conv, norm_layer_args={\"num_features\": out_channels[i]},\n",
    "                        activation_function=nn.ReLU,\n",
    "                        pooling_layer=pool_type, pooling_layer_args=\n",
    "                        {\"kernel_size\":pool_kernel_size[i], \"padding\":pool_padding[i]})\n",
    "        )\n",
    "        \n",
    "\n",
    "        length.append(int(sequential[i].output_shape(length[i])['L_out']))\n",
    "        in_channels.append(out_channels[i])\n",
    "    \n",
    "    sequential = sequential.append(nn.Flatten(1, 2))\n",
    "    \n",
    "    first_input = out_channels[-1]*length[-1]\n",
    "        \n",
    "    n_dense_layers = trial.suggest_int(\"n_dense_layers\", 1, 3)\n",
    "    \n",
    "    dense_layers_sizes = [first_input]\n",
    "        \n",
    "    batch_norm_dense = trial.suggest_categorical(\"batch_norm_dense\", [None, nn.BatchNorm1d])\n",
    "    \n",
    "    for i in range(n_dense_layers):\n",
    "        \n",
    "        dense_layers_sizes.append(\n",
    "            trial.suggest_int(\"dense_layer_size{}\".format(i+1), 1, dense_layers_sizes[i]))\n",
    "        \n",
    "        sequential = sequential.append(\n",
    "            LinearBlock(in_features=dense_layers_sizes[i], out_features=dense_layers_sizes[i+1],\n",
    "                        norm_layer=batch_norm_dense, norm_layer_args={\"num_features\": dense_layers_sizes[i+1]},\n",
    "                        activation_function=nn.ReLU))\n",
    "        \n",
    "    sequential = sequential.append(nn.Linear(dense_layers_sizes[-1], 1))\n",
    "    \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
    "    weight_decay = trial.suggest_uniform(\"weight_decay\", 0, 1e-5)\n",
    "    \n",
    "    plateau_factor = trial.suggest_uniform(\"plateau_factor\", 0, 1)\n",
    "    \n",
    "    m = Reg(pytorch_module=sequential,\n",
    "            loss_func=nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam, optimizer_args={\"lr\": lr, \"weight_decay\": weight_decay},\n",
    "            lr_scheduler=ReduceLROnPlateau,\n",
    "            lr_scheduler_args={\"mode\": 'min', \"factor\": plateau_factor, \"patience\": 5, \"threshold\": 1e-3},\n",
    "            tb_dir=\"CNN1D_no\")\n",
    "    \n",
    "    trainer = L.Trainer(max_epochs=NUM_EPOCH,\n",
    "                        enable_progress_bar = False, log_every_n_steps=1,\n",
    "                        accelerator=\"auto\", devices=1, callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")], logger = True)\n",
    "    trainer.fit(m, datamodule=dm)\n",
    "\n",
    "    return_float = trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    trainer.test(m, datamodule=dm)\n",
    "\n",
    "    return return_float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Acnodnem-Ailiram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
